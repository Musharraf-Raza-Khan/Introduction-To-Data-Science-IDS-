![Screenshot 2023-04-14 133113](https://user-images.githubusercontent.com/95965896/231989826-da4cf2fa-a2d6-4ba9-aa37-6768401dd21d.png)

![Aspose Words d8afd77f-df63-475f-bc94-ec1700bdbba8 001](https://user-images.githubusercontent.com/95965896/234596262-f345b534-9563-45be-b2be-534085e18dc5.png)

# Introduction-To-Data-Science-IDS-
Welcome To My (IDS) Repository &amp; Happy Learning !

**SESSIONAL MARKS DIVISION**
- 05 Marks For Quizes.
- 10 Marks Professional Well-Maintained Github Repository With Self-Explanatory Code & Readme Files. 
- 10 Marks For Assignments.


|**GITHUB (RES)**|**INTRODUCTION TO DATA SCIENCE (IDS)**|
| :-: | :- |

**GITHUB REPOSITORY** 

**SOFTWARE ENGINEERING** 

**DEPARTMENT**

![Aspose Words d8afd77f-df63-475f-bc94-ec1700bdbba8 002](https://user-images.githubusercontent.com/95965896/234596382-b4add1b0-7305-4c49-8b99-7afed6d34e89.png)


**INTRODUCTION TO DATA SCIENCE**

**(IDS)** 

**BY**

**MUSHARRAF RAZA KHAN | 52024**

BS-SE 8<sup>th</sup> 

**INSTRUCTOR** 

**MA’AM SANIYA ASHRAF**

Lecturer, Department of Software Engineering,

Balochistan University of Information Technology, Engineering & Management Sciences, (BUITEMS) Quetta.

“Session Spring-2023”

**COURSE OVERVIEW:**

Data science is an interdisciplinary field that uses statistical and computational methods to extract insights and knowledge from data. It involves the entire data lifecycle, from data collection and storage to analysis and visualization. 

The goal of data science is to discover patterns and relationships in data that can be used to inform business decisions, improve products and services, and solve real-world problems. It combines techniques from statistics, machine learning, and computer science to transform raw data into actionable insights. 

In today's data-driven world, data science has become a critical component of many industries and is used to solve a wide range of problems, from predicting customer behaviour to optimizing supply chain operations.

**DATA SCIENCE:**

![image](https://user-images.githubusercontent.com/95965896/234592377-22a5a22e-f5b6-4055-b7fb-75093e7b6d0d.png)

Data science is an interdisciplinary field that involves using scientific methods, algorithms, and tools to extract insights and knowledge from structured and unstructured data. It combines various fields such as statistics, computer science, mathematics, and domain expertise to extract useful information from data.


**KEY ELEMENTS OF DATA SCIENCE:**

- Statistical Analysis, 
- Data Modeling, 
- Machine Learning, 
- Data Visualization etc.





**THE IMPORTANCE OF DATA SCIENCE**

![image](https://user-images.githubusercontent.com/95965896/234591555-6765aa49-459f-4f0b-9466-d5fa9ea9fcae.png)


Description automatically generated](Aspose.Words.d8afd77f-df63-475f-bc94-ec1700bdbba8.004.png)

- Better Decision Making
- Predictive Analytics:
- Personalization:
- Cost Reduction:
- Healthcare:

Overall, data science has become a critical tool for organizations to extract insights and knowledge from data, make informed decisions, and stay competitive in today's world.

**EXAMPLES OF INDUSTRIES THAT USE DATA SCIENCE:**

![image](https://user-images.githubusercontent.com/95965896/234591646-e771543d-49a6-491a-aea9-d5285ac72e88.png)

- E-commerce:
- Healthcare:
- Finance:
- Marketing:	
- Transportation:
- Energy:
- Education:

Overall, data science has become an integral part of various industries, enabling organizations to extract valuable insights from data and make informed decisions.

**DATA SCIENCE PROCESS**

![image](https://user-images.githubusercontent.com/95965896/234591742-3d5f7426-acc9-48a2-bfed-c31994bab0b6.png)

Description automatically generated](Aspose.Words.d8afd77f-df63-475f-bc94-ec1700bdbba8.006.png)

The data science process 

- Problem Definition:
- Data Collection:
- Data Cleaning and Preparation:
- Data Exploration and Analysis:
- Model Building:
- Model Evaluation and Validation:
- Deployment and Monitoring:

Overall, the data science process is iterative and involves continuous refinement and improvement to ensure that the models provide actionable insights and value to the business or research question at hand.

**DATA SCINCE TOOLS & TECHNOLOGIES**

**Programming Languages:** 

Data scientists typically use programming languages such as Python, R, and SQL to manipulate, analyze, and visualize data.

**Tools For Data Cleaning & Preprocessing:**

Pandas, Open Refine, etc.

**Tools For Statistical Analysis & Modeling:**

Numpy, Scipy, etc.

**Machine Learning Frameworks:**

Machine learning frameworks such as TensorFlow, Keras, and Scikit-Learn are used to develop and train machine learning models.

**Tools For Data Visualization:** 

Data visualization tools such as Tableau, Power BI, and Matplotlib are used to create visual representations of data to help understand patterns, trends, and insights

**SKILLS REQUIRED FOR CAREER IN A DATA SCIENCE**

A career in data science requires a combination of technical and soft skills. Here are some of the key skills that are essential for a career in data science:

**Strong Background in Mathematics & Statistics:** Data scientists need a solid foundation in mathematical concepts such as linear algebra, calculus, and probability theory.

**Proficiency in Programming Languages:** Data scientists need to be proficient in programming languages such as Python, R, and SQL to manipulate, analyze, and visualize data.

**Data Wrangling & Cleaning**: Data scientists need to be skilled in cleaning and wrangling data to ensure it is accurate, complete, and in a usable format.

**Machine Learning & Data Modeling:** Data scientists should have a strong understanding of machine learning algorithms and data modeling techniques such as regression, classification, clustering, and neural networks.

**Data Visualization:** Data scientists should be able to create meaningful and effective visualizations of data to communicate insights to stakeholders.

**Communication & Collaboration:** Data scientists need to be able to communicate complex technical concepts to non-technical stakeholders and work collaboratively with other teams within an organization.

**Domain Expertise:** Depending on the industry or field, data scientists may need to have domain expertise in areas such as finance, healthcare, marketing, or engineering.

Overall, data scientists need a combination of technical and soft skills to be successful in their careers. They need to be able to work with large and complex datasets, apply statistical and machine learning techniques, and communicate insights effectively to stakeholders.

![](Aspose.Words.d8afd77f-df63-475f-bc94-ec1700bdbba8.007.png)

**DATA SCINCE CAREER OPPORTUNITIES**

Data science is a rapidly growing field with a wide range of career opportunities across industries. Here are some of the common career paths in data science:

**Data Scientist:** Data scientists use statistical and machine learning techniques to extract insights from data and develop models to solve business problems.

**Data Analyst:** Data analysts use data to identify trends, patterns, and insights to inform business decisions.

**Business Intelligence Analyst:** Business intelligence analysts create dashboards and reports to provide insights on business performance and trends.

**Machine Learning Engineer:** Machine learning engineers develop and deploy machine learning models and algorithms to solve complex business problems.

**Data Engineer:** Data engineers design and build systems to store and manage large datasets for analysis.

**Data Architect:** Data architects design and maintain the overall data architecture of an organization to ensure that data is stored, accessed, and used effectively.

**Data and Analytics Consultant:** Data and analytics consultants provide strategic guidance to organizations on how to use data to improve business performance.

**Data Journalist:** Data journalists use data to tell stories and provide insights on current events and issues.

Overall, there are many different career paths in data science, each with unique opportunities for growth and development. The demand for data science professionals is expected to continue to grow as more and more organizations realize the value of data-driven decision-making.




**PYTHON PROGRAMMING LANGUAGE FUNCTIONS**

![image](https://user-images.githubusercontent.com/95965896/234592505-5471f380-63ce-4a59-a0c6-f40d1be021c5.png)

Description automatically generated](Aspose.Words.d8afd77f-df63-475f-bc94-ec1700bdbba8.008.png)

Python is a versatile programming language that supports a wide range of functions. Here are some of the common functions in Python:

**Print Function:** The print() function is used to display output to the console.

**Input Function:** The input() function is used to get input from the user via the keyboard.

**Mathematical Functions:** Python supports a wide range of mathematical functions such as abs(), round(), min(), max(), pow(), and sqrt().

**String Functions:** Python supports string functions such as len(), str(), upper(), lower(), replace(), split(), and join().

**List Functions:** Python supports list functions such as append(), insert(), remove(), pop(), index(), count(), sort(), and reverse().

**Dictionary Functions:** Python supports dictionary functions such as keys(), values(), items(), get(), and update().

**Control Flow Functions:** Python supports control flow functions such as if/else statements, loops (for and while), break and continue statements, and try/except blocks.

**File I/O Functions:** Python supports file I/O functions such as open(), read(), write(), close(), and seek().

These are just some of the common functions in Python. Python is a very powerful and flexible language that supports many other functions as well.

**PYTHON PROGRAMMING LANGUAGE TYPES AND SEQUENCES**

In Python, there are several built-in data types and sequences that allow for easy manipulation and storage of data. Here are some of the common types and sequences in Python:

**Numeric Types:** Python supports three types of numeric data: integers, floating-point numbers, and complex numbers.

**String Type:** Strings in Python are sequences of characters enclosed in single or double quotes.

**Boolean Type:** The Boolean data type represents truth values, either True or False.

**List Sequence:** Lists are ordered collections of items enclosed in square brackets []. Lists can contain any data type, including other lists.

**Tuple Sequence:** Tuples are similar to lists, but are immutable (i.e., their values cannot be changed). Tuples are enclosed in parentheses ().

**Set Sequence:** Sets are unordered collections of unique elements enclosed in curly braces {}. Sets can be used to perform mathematical set operations such as union, intersection, and difference.

**Dictionary Sequence:** Dictionaries are unordered collections of key-value pairs enclosed in curly braces {}. Each key is associated with a value, and keys must be unique.

**Range Sequence:** The range() function is used to create a sequence of numbers. It takes three arguments: start, stop, and step.

These are just some of the common data types and sequences in Python. Python also supports many other data types and sequences, including **bytearrays, frozensets**, and **namedtuples.**

**READIND COMMA SEPERATED VALUES (CSV) FILES IN PYHTON**

In Python, you can read and write CSV (Comma-Separated Values) files using the built-in csv module. **Example** of how to read and write CSV files in Python:

**Reading CSV Files:**

In Python, you can read and write CSV (Comma-Separated Values) files using the built-in csv module. **Example** of how to read and write CSV files in Python:

**CODE**

<a name="_hlk132369216"></a>**Print(Musharraf Raza Khan | 52024)**

import csv

with open('file.csv') as csv\_file:

`    `csv\_reader = csv.reader(csv\_file)

`    `for row in csv\_reader:

`     `print(row)

**EXP:** In this example, we open a CSV file called 'file.csv' using the open() function and read its contents using the csv.reader() function. We then iterate through each row in the CSV file using a for loop and print each row to the console.


**WRITING COMMA SEPERATED VALUES (CSV) FILES**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import csv

data = [['Name', 'Age', 'Gender'], ['John', 25, 'Male'], ['Jane', 30, 'Female']]

with open('new\_file.csv', mode='w', newline='') as csv\_file:

` `csv\_writer = csv.writer(csv\_file)

` `for row in data:

`  `csv\_writer.writerow(row)

**EXP:** In this example, we create a list of lists called data which contains some sample data. We then open a new CSV file called 'new\_file.csv' using the open() function and write the contents of data to the CSV file using the csv.writer() function. We use a for loop to iterate through each row in data and write it to the CSV file using the csv\_writer.writerow() function.

**OBJECTS AND MAP IN PYTHON**

![image](https://user-images.githubusercontent.com/95965896/234592568-575f7d44-4ebf-4c8b-a26e-a7f7cff3b7e4.png)


Description automatically generated](Aspose.Words.d8afd77f-df63-475f-bc94-ec1700bdbba8.009.png)

In Python, objects and maps are related to the concepts of classes and dictionaries, respectively.

**Objects:**

In Python, everything is an object. An object is an instance of a class, which defines a set of attributes and methods. To create an object, you first define a class, which serves as a blueprint for creating objects. 

**Example**:

**CODE**

**Print(Musharraf Raza Khan | 52024)**

class Person:

`    `def \_\_init\_\_(self, name, age):

`        `self.name = name

`        `self.age = age

p1 = Person("John", 25)

print(p1.name)

print(p1.age)

**EXP:** In this example, we define a class called Person which has two attributes: name and age. We then create an object of the Person class called p1, passing in values for the name and age attributes. We can then access the values of the attributes using the dot notation (i.e., p1.name and p1.age).

**MAPS**

In Python, maps are commonly implemented using dictionaries. A dictionary is a collection of key-value pairs, where each key is unique and associated with a value. 

**Example**:

**CODE**

**Print(Musharraf Raza Khan | 52024)**

my\_dict = {"key1": "value1", "key2": "value2", "key3": "value3"}

print(my\_dict["key2"])

**EXP:** In this example, we define a dictionary called my\_dict which contains three key-value pairs. We can access the value associated with a specific key using the square bracket notation (i.e., my\_dict["key2"] returns the value "value2").

Note that dictionaries can also be created using the dict() constructor function, and can contain keys of different data types.

**LAMBDA AND LIST COMPREHENSIONS**

Lambda functions and list comprehensions are two powerful features of Python that allow for concise and expressive **CODE**.

**Lambda Functions:**

Lambda functions, also known as anonymous functions, are functions that are defined without a name. They are useful for defining small, one-time-use functions. 

**Example:**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

\# Define a lambda function that squares a number

square = lambda x: x\*\*2

\# Call the lambda function

result = square(5)

print(result)

**EXP:** In this example, we define a lambda function called square that takes a single argument and returns the square of that argument. We can then call the lambda function like a regular function and pass in an argument (in this case, the number 5).

**LIST COMPREHENSIONS**

List comprehensions are a concise way to create lists in Python. They are essentially a compact for loop that performs some operation on each element in a sequence. 

**Example:**

**CODE**

**Print(Musharraf Raza Khan | 52024)**
\# Create a list of the first 10 square numbers

squares = [x\*\*2 for x in range(1, 11)]

print(squares)

**EXP:** In this example, we use a list comprehension to create a list called squares that contains the first 10 square numbers. The list comprehension consists of a for loop that iterates over the range 1 to 10, and an expression that squares each number in the range.

**Numpy:**

![image](https://user-images.githubusercontent.com/95965896/234592829-0af00e5c-ba4c-4480-9879-d59d2f9abc72.png)


NumPy is a Python library used for working with arrays. It stands for Numerical Python. NumPy provides a fast and efficient way to work with arrays in Python and is an essential tool in data science and scientific computing.

Overall, NumPy is a powerful and versatile library that is essential for many scientific computing and data analysis tasks in Python.

**ARRAY CREATION NUMPY:**

NumPy provides several ways to create arrays. Here are some common ways to create arrays using NumPy:

1. **Creating An Array From A List:**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import numpy as np

\# Create an array from a list

arr1 = np.array([1, 2, 3, 4, 5])

print(arr1)

1. **Creating A Multi Dimensional Array:**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import numpy as np

\# Create a 2D array from a list of lists

arr2 = np.array([[1, 2, 3], [4, 5, 6]])

print(arr2)

1. **Creating An Array of Zeros:**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import numpy as np

\# Create an array of zeros with shape (3, 4)

arr3 = np.zeros((3, 4))

print(arr3)

1. **Creating An Array Of Ones:**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import numpy as np

\# Create an array of ones with shape (2, 3, 4)

arr4 = np.ones((2, 3, 4))

print(arr4)

1. **Creating An Array With A Range f Values:**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import numpy as np

\# Create an array with values ranging from 0 to 9

arr5 = np.arange(10)

print(arr5)

1. **Creating An Array With A Specific Number of Evenly Spaced Values:**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import numpy as np

\# Create an array with 5 evenly spaced values between 0 and 1

arr6 = np.linspace(0, 1, 5)

print(arr6)

**ARRAY OPERATION**

![image](https://user-images.githubusercontent.com/95965896/234592920-3ea5e2fd-4619-43d2-a988-cec8955b3d11.png)

NumPy arrays allow for fast and efficient operations on arrays, including mathematical operations, logical operations, and statistical operations. Here are some common array operations in NumPy:

**Mathematical Operations**: NumPy provides a large number of mathematical functions that can be applied to arrays, including addition, subtraction, multiplication, division, exponentiation, and more. These functions can be applied to entire arrays or to individual elements.

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import numpy as np

**# Create two arrays**

arr1 = np.array([1, 2, 3])

arr2 = np.array([4, 5, 6])

**# Add the two arrays**

arr3 = arr1 + arr2

print(arr3)

**# Multiply the two arrays**

arr4 = arr1 \* arr2

print(arr4)

**Logical Operations:** NumPy provides logical operators such as and, or, and not that can be used to perform logical operations on arrays.

import numpy as np

**# Create an array**

arr = np.array([True, False, True])

\# Apply a logical operator to the array

result = np.logical\_not(arr)

print(result)

**Statistical Operations:** NumPy provides a large number of statistical functions that can be used to analyze arrays, including mean, median, standard deviation, variance, and more.

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import numpy as np

**# Create an array**

arr = np.array([1, 2, 3, 4, 5])

**# Calculate the mean of the array**

mean = np.mean(arr)

print(mean)

**# Calculate the standard deviation of the array**

std = np.std(arr)

print(std)

**INDEXING**

![image](https://user-images.githubusercontent.com/95965896/234593031-76c5f9ca-20d5-40b4-96cd-fbb28f499fa0.png)

**Indexing:** Indexing is the process of accessing a specific element in a data structure by its position, or index, in the sequence. In Python, the first element in a sequence has an index of 0, and the last element has an index of -1. You can use square brackets [] to index a sequence in Python.

**CODE**

**# Indexing a list**

**Print(Musharraf Raza Khan | 52024)**

my\_list = [1, 2, 3, 4, 5]

print(my\_list[0])   # Output: 1

print(my\_list[-1])  # Output: 5

**CODE**

**# Indexing a string**

**Print(Musharraf Raza Khan | 52024)**

my\_string = "hello"

print(my\_string[1])   # Output: 'e'

print(my\_string[-1])  # Output: 'o'



**SLICING**

**Slicing:** Slicing is the process of extracting a subset of elements from a data structure. In Python, you can slice a sequence using the syntax [start:stop:step], where start is the starting index, stop is the stopping index (exclusive), and step is the step size between each element.

**# Slicing a list**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

my\_list = [1, 2, 3, 4, 5]

print(my\_list[1:4])    # Output: [2, 3, 4]

print(my\_list[::2])    # Output: [1, 3, 5]

**# Slicing a string**

**CODE**

**Print(Musharraf Raza Khan | 52024)**

my\_string = "hello"

print(my\_string[1:4])  # Output: 'ell'

print(my\_string[::2])  # Output: 'hlo'

**ITERATING**

Iterating: Iterating is the process of looping over the elements in a data structure, one by one. In Python, you can use a for loop to iterate over a sequence.

**CODE**

**# Iterating over a list**

**Print(Musharraf Raza Khan | 52024)**

my\_list = [1, 2, 3, 4, 5]

for element in my\_list:

` `print(element)

**CODE**

**# Iterating over a string**

**Print(Musharraf Raza Khan | 52024)**

my\_string = "hello"

for character in my\_string:

` `print(character)

**NOTE:** Indexing, slicing, and iterating are fundamental operations in Python and are used extensively in data analysis and scientific computing. NumPy arrays, for example, can also be indexed, sliced, and iterated over in similar ways to Python sequences.

**NUMPY WITH DATASET**

NumPy can be used with datasets in a variety of ways. Here are a few examples:

**Loading Data Into NumPy Arrays:** You can use NumPy's loadtxt function to read data from a text file into a NumPy array. The function can handle different formats, such as comma-separated values (CSV) or tab-separated values (TSV), and can also skip header lines or handle missing data.

import numpy as np

data = np.loadtxt('data.csv', delimiter=',', skiprows=1)

**Performing Operations On Arrays:** NumPy provides many functions for working with arrays, such as performing arithmetic operations, calculating statistical measures, and manipulating the shape of arrays. For example, you can calculate the mean and standard deviation of an array using the mean and std functions:

mean = np.mean(data)

std = np.std(data)

**Filtering & Selecting Data:** You can use NumPy arrays to filter and select data based on certain conditions. For example, you can select all rows where a certain column is greater than a certain value:

selected\_rows = data[data[:, 2] > 10]

**Visualizing Data:** NumPy arrays can also be used with other Python libraries for data visualization, such as Matplotlib or Seaborn. You can create plots and charts to explore patterns and trends in the data:

**CODE**

**Print(Musharraf Raza Khan | 52024)**

import matplotlib.pyplot as plt

plt.scatter(data[:, 0], data[:, 1])

plt.xlabel('X')

plt.ylabel('Y')

plt.show()

**NOTE:** These are just a few examples of how NumPy can be used with datasets. NumPy provides many more features and functions for working with arrays, making it a valuable tool for data analysis and scientific computing.

**26.April.2023**

**API:**

![image](https://github.com/Musharraf-Raza-Khan/Introduction-To-Data-Science-IDS-/assets/95965896/815ca14a-7655-4429-88b7-c796c4368c18)

API stands for Application Programming Interface. Different software applications can communicate and interact with one another according to a set of rules and protocols. APIs specify the procedures, data structures, and guidelines that programmers must adhere to when connecting their applications with a certain piece of software or service.

Without having to comprehend the intricate implementation details, APIs allow developers to access and utilise the functionality of other software systems, such as web services, libraries, or operating systems. They give programmes a standardised way to ask for and exchange data, carry out actions, and use resources made available by the API provider.

In many diverse situations, including web development, mobile app development, cloud computing, and the integration of numerous software systems, APIs are frequently utilised. By building on top of current platforms and technologies, they enable developers to take use of existing services, expand the functionality of their apps, and generate novel solutions.

**API Working:**

![image](https://github.com/Musharraf-Raza-Khan/Introduction-To-Data-Science-IDS-/assets/95965896/3fff117b-0d42-408f-93af-17d0c83a728a)

An API functions in a set of steps that enable communication and interaction between programmes. Here is a general explanation of how APIs operate:

-Request: The client application sends an HTTP request (such as GET, POST, PUT, or DELETE) to a particular URL or endpoint to start a request to the API. The request contains data in the request body occasionally, as well as headers and parameters.

-Routing: Based on the URL and HTTP method used, the API server receives the request and chooses the best endpoint and handler to handle it.
Processing: The requested endpoint's associated activities or business logic are carried out by the API server. This could entail using external services, conducting calculations, accessing databases, or any other necessary tasks.

-Authentication and Authorization: The server may authenticate the client to make sure it has the necessary permissions to access the requested resources, depending on the security needs of the API. Use of API keys, tokens, or other authentication methods may be required.

-Data retrieval or manipulation: Depending on the parameters of the request and the business logic, the API server retrieves or modifies the data. This can entail running a database query, handling files, or carrying out any other relevant operations.

-Response: After handling the request, the server creates an HTTP response. The answer normally consists of a response body containing the requested data or pertinent error messages, as well as a status code indicating whether the request was successful or unsuccessful.

-Data transfer: Over the network, the server delivers the HTTP response back to the client application. The client application receives and processes the response.

-Client-side processing: After receiving the response, the client application processes it in accordance with its needs. This may entail analysing the response, showing the user the data, or taking additional actions in response to the information received.

 The use of APIs allows programmes to communicate with one another, share information, and take advantage of the functions offered by the API provider. The architecture of the API will determine the specifics and protocols used for communication, such as REST (Representational State Transfer), SOAP (Simple Object Access Protocol), GraphQL, or others.

**Data Extraction From API Through Code?**

You normally need to perform the following steps in order to extract data from an API using code:

1. Select an API: Choose the API from which you can extract the desired data. This could be an internal API you created or a public API offered by a platform or service.

2. Obtain API credentials. You might need to acquire authentication credentials, such as an API key, access token, or username/password combination, depending on the API's specifications. Your queries to the API are authenticated using these credentials.

3. Configure the API client: You must configure either an HTTP client library or a specialised API client library, depending on the programming language you're using. These tools make it easier to send HTTP requests and manage responses.

4. Send HTTP requests: To send HTTP queries to the API's endpoints, use the API client library. Using the API documentation and the action you wish to do (such as accessing, creating, or changing data), choose the proper HTTP method (GET, POST, PUT, or DELETE). Include any relevant request body data, headers, and parameters.

5. Handle the response: You can extract the data from the response body after receiving the response from the API. The information could be delivered in a number of formats, like JSON.

**Example Python Code For Data Extraction Through API** 
import requests

// API endpoint URL

url = "https://api.example.com/data"

//API request headers (if required)

headers = 
{
    "Authorization": "Bearer YOUR_ACCESS_TOKEN",
    "Content-Type": "application/json"
}

//Send GET request to the API

response = requests.get(url, headers=headers)

//Check if the request was successful (200 status code)

if response.status_code == 200:
    # Extract the response data in JSON format
    data = response.json()

    # Process and use the extracted data as needed
    for item in data:
        print(item)
else:
    # Request was not successful, print the error status code
    print("Error:", response.status_code)

**Explaination of The Code:**

Certainly! Let's go through the code step by step to understand what it does:

1. We import the requests library, enabling us to make HTTP queries to the API and manage their answers.

2. We use a string variable called url to represent the API endpoint URL. This need to be the URL that the API documentation provides for the particular data that you want to get.

3. To hold any necessary headers for the API request, we then build a dictionary called headers. For authentication purposes, we add a bearer token and a "Authorization" header in the example code. Depending on the type of authentication the API requires, you might need to change this.

4. To submit a GET request to the API endpoint, we employ the requests.get() method. The endpoint URL is specified by the url variable, and any necessary headers are passed via an argument in the form of the headers dictionary.

5. The response variable contains the API response that was received.

6. Using response.status_code, we examine the response's status code. We explicitly check in this case if it equals 200, which denotes a successful request. We extract and handle the data if the status code is 200.

7. Using the response.json() method, the response's content is retrieved and parsed as JSON. This implies that JSON data is returned by the API. You would need to modify the code if the API returned data in a different format.

8. We iterate over the extracted data using a for loop and print each item. This is just an example to demonstrate how to process the data. You would typically customize this part of the code based on your specific requirements.

9. If the status code is not 200, the code prints an error message along with the actual status code received.

# NOTE:
Please note that this is a simplified example, and you may need to modify the code based on the specific requirements of the API you are working with, such as authentication, request parameters, error handling, and data processing. Refer to the API documentation for accurate implementation details.

In this illustration, a GET request is made using the requests library and sent to the API endpoint identified by the url variable. Any necessary headers, like an access token for authentication, are contained in the headers dictionary. Depending on the particular requirements of the API you are using, you might need to adjust the headers dictionary.

After receiving the response, we verify that the status code is 200, which denotes a successful request. If so, we use the.json() method to extract the response data in JSON format. The retrieved data can subsequently be processed and used in accordance with your demands.

The code will print the error status code if the request was unsuccessful (status code other than 200).

Before running the code, make sure to install the requests library. Pip install requests can be used to accomplish this in your terminal or command prompt.

25.May.2023

**Exploratory Data Analysis (EDA)**

Exploratory Data Analysis (EDA) is a method for examining and summarising data sets in order to better understand their key features, find trends, spot abnormalities, and develop ideas for additional investigation. In order to explore the data and generate insights that might inform later data modelling and decision-making processes, it entails applying a variety of statistical and visual tools.

The following are some typical procedures and methods used in exploratory data analysis:

1. Data gathering and initial inspection: Compile the pertinent data and look over its dimensions, structure, and fundamental characteristics including the number of variables, data types, and missing values.

2.Data cleaning and preprocessing: Handle missing values, outliers, and inconsistencies in the data during data cleaning and preprocessing. In this step, missing values may be imputed, outliers may be eliminated, and variable transformations may be necessary.

3.Univariate analysis: Analyse each variable separately to comprehend its distribution, summary statistics (mean, median, and mode), range, and spot outliers using a univariate approach. The usage of methods like histograms, box plots, and summary statistics is widespread.

4.Bivariate and multivariate analysis:Analyse the associations between variables using bivariate and multiple variables. Correlation analysis can be used with numerical variables to express the strength and direction of correlations. It is common to employ scatter plots, heatmaps, and correlation matrices. Contingency tables and bar plots can be used to investigate associations for categorical variables.

5.Visualisation: To exhibit the data and spot patterns, trends, and outliers, use a variety of visualisations such scatter plots, line plots, bar charts, histograms, box plots, and heatmaps. Making data-driven judgements and comprehending complex relationships can both benefit from visualisations.

6.Feature Engineering: Feature engineering is the process of creating new features or altering existing ones using domain expertise and understanding derived from EDA. To enhance the performance of the model, this stage may involve generating derived features, scaling variables, or introducing interaction terms.

7.Summary and interpretation: Condense the results of the EDA process, highlight significant trends or insights, and develop ideas for more research or modelling. In this step, the results are interpreted in light of the problem domain.

# NOTE:
As it aids in understanding the data, identifies potential problems, and directs further analysis or modelling tasks, EDA is a vital step in the data analysis pipeline. It enables data scientists and analysts to comprehend the data more thoroughly, spot problems with the quality of the data, and produce ideas for additional research.

27.May.2023

**MUSHARRAF RAZA KHAN | 52024 | BUITEMS**
